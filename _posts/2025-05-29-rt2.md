---
title: "[RT-2] Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
date: 2025-05-29 21:00:00 +0900
description: ""
image:
  path: /assets/img/favicons/favicon-96x96.png
  alt: "RT-2"
categories: [Embodied AI, 논문읽기]
tags: [Embodied AI, 논문읽기]
---

> 이 글은 [RT-2](https://arxiv.org/abs/2307.15818) 논문의 내용을 정리한 글입니다.

---

# 본문

## Abstract

본 논문은 인터넷 규모의 데이터로 학습된 비전-언어 모델을 종단 간(end-to-end) 로봇 제어에 통합함으로써,  일반화성능을 높이고 창발적(emergent) 의미론적(sementic) 추론을 가능하게 하는 방법을 연구한다.

목표: 단일 종단 간 모델이 로봇의 관찰(observation)을 행동(action)으로 매핑하는 방법을 학습하는 것이다.

방법: 로봇 궤적 데이터뿐 아니라 시각적 질문-답변(VQA) 등 다양한 비전, 언어 작업을 포함해 최첨단 비전-언어 모델을 로봇 데이터와를 학습하면서 미세조정할 것을 제안함 

이를 위해 로봇의 행동을 텍스트 토큰으로 표현하여, 자연어 토큰과 동일한 방식으로 모델 학습에 포함시킨다. 이 접근을 비전-언어-행동 모델(VLA)이라 하며, RT-2는 그중 하나의 구체적 예시이다.

평가: 약 6,000개의 시험을 통해 이루어졌으며, RT-2는 로봇 학습에 대해 다음과 같은 향상된 능력을 보였다. 

1. 새로운 개체에 대한 일반화 능력 향상 
2. 훈련 데이터에 없는 명령어 해석 (예: 특정 숫자나 아이콘에 따라 물체 배치) 
3. 사용자의 명령에 대한 기본적인 의미 추론 수행 (예: 가장 작은 물체, 가장 가까운 물체 등 인식 후 집기) 
4. 연쇄적 사고(chain of thought)를 통한 다단계 추론 수행 (예: ‘망치로 쓸 수 있는 물체 집기’ → ‘바위 집기’, ‘피곤할 때 마시는 음료’ → ‘에너지 음료’ 등)

## 1. 서론

웹에서 수집한 대규모 데이터를 바탕으로 사전 학습된 거대 모델은 다양한 문제에 폭넓게 적용될 수 있는 강력한 기반(플랫폼)이다.

### 대규모 언어 모델(LLM)

유창한 텍스트 생성 능력  
RT-1(2022) https://arxiv.org/abs/2212.06817  
open AI(2023) https://arxiv.org/abs/2112.01511  
palm2(2023) https://arxiv.org/abs/2305.10403

새로운 문제 해결 능력  
Cobbe(2021) https://arxiv.org/abs/2110.14168  
Lewkowycz(2022) https://arxiv.org/abs/2206.14858  
Polu(2022) https://arxiv.org/abs/2202.01344

창조적인 문장 생성 능력  
Brown(2020) https://arxiv.org/abs/2005.14165  
GPT-4(2023) https://arxiv.org/abs/2303.08774

코드 사용 능력
Chen(2021) https://arxiv.org/abs/2107.03374

### 시각 언어 모델(VLM)

개방형 어휘 시각적 인식  
Radford(2021) https://arxiv.org/abs/2103.00020  
Minderer(2022) https://arxiv.org/abs/2205.06230  
Kirillov(2023) https://arxiv.org/abs/2304.02643

이미지의 객체-행위자 상호 작용에 대한 복잡한 추론  
Alayrac(2022) https://arxiv.org/abs/2204.14198  
Hao(2022) https://arxiv.org/abs/2206.06336  
Wang(2022) https://arxiv.org/abs/2205.14100  
Chen(2023) https://arxiv.org/abs/2209.06794  
Driess(2023) https://arxiv.org/abs/2303.03378  
Huang(2023) https://arxiv.org/abs/2302.14045

이러한 모델들의 의미론적 추론, 문제 해결 및 시각적 해석 능력은 실제 환경에서 다양한 작업을 수행해야 하는 범용 로봇에 매우 유용할 것이다. 그러나 로봇이 이러한 능력을 어떻게 획득해야 하는지가 불분명하다. 특히 웹 기반 LLM/VLM은 수십억 개의 토큰과 이미지로 학습되지만, 로봇의 경우 수백만 번의 실제 상호작용을 수집하는 것은 현실적으로 어렵기 때문이다.

이러한 모델(LLM이나 VLM)을 로봇에 직접 적용하는 것은 어려운 작업이다. 예를 들어, LLM은 주로 텍스트나 테이블 기반 추론에 특화되어 있지만, 로봇은 좌표계에서 이동하기 위해 저수준의 엔드 이펙터 명령을 필요로 한다.

최근 LLM과 VLM을 로봇 공학에 통합하려고 시도한 논문  
Alayrac(2022) https://arxiv.org/abs/2204.14198  
Driess(2023) https://arxiv.org/abs/2303.03378  
Vemprala(2023) https://arxiv.org/abs/2306.17582

이전의 방법은 일반적으로 로봇 계획의 고수준 측면만 다루며, 본질적으로 명령을 해석하고 이를 개별 프리미티브(객체 선택 및 배치)로 구문 분석하는 상태머신의 역할만을 수행하며, 이는 훈련에서 얻을 수 있는 풍부한 의미론적 지식의 이점을 얻지 못하는 저수준 컨트롤러에 의해 실행된다. 

> 이에 본 논문은 다음과 같은 핵심 질문을 제시한다: 사전 학습된 대규모 비전-언어 모델을 저수준 로봇 제어에 직접 통합함으로써 일반화 능력과 창발적 의미론적 추론을 가능하게 할 수 있을까?

![figure1](/assets/img/RT-2/figure1.png)

<그림 1> RT-2 개요
**로봇 행동을 텍스트 토큰으로 변환**하여, 기존의 비전-언어 데이터와 통합된 방식으로 학습한다. 학습 시에는 이러한 텍스트 토큰들이 일반적인 자연어 응답처럼 취급되고, 추론 시에는 해당 토큰이 실제 로봇 행동으로 디코딩되어 폐쇄 루프 제어를 수행한다. 이러한 방식을 통해, 웹으로부터 사전 학습된 비전-언어 모델의 의미론적 지식과 일반화 능력을 로봇 행동 정책에 직접적으로 활용할 수 있게 된다.

방법론: 개방형 어휘 시각적 질의응답 및 시각적 대화를 위해 설계된 시각 언어 모델을 직접 학습시켜 저수준 로봇 동작을 출력함과 동시에 시각 언어 과제를 해결한다. 원래의 LLM, VLM은 일반적으로 자연어 토큰을 생성하도록 학습되지만, *동작을* 텍스트 토큰으로 토큰화 하고 카메라 관찰과 결합된 로봇 명령에 "응답"하여 상응하는 동작을 생성하는 "다중 모달 문장" (Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) )을 생성함으로써 로봇 궤적을 학습시킬 수 있다. 로봇 정책에 VLM을 통합하기 위한 이전 대안 (Shridhar et al., [2022a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib69) ) 이나 처음부터 새로운 비전-언어-동작 아키텍처를 설계하는 것 (Reed et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib64) ) 과 대조적임. 이미 학습된 기존 비전-언어 모델을 새로운 매개변수 없이 훈련하여 텍스트 인코딩된 동작을 출력함. 이런 모델을 비전-언어-동작(VLA) 모델이라고 하겠음.

RT-2는 기존 연구와 유사한 데이터 세트를 사용하지만 대규모 비전-언어 백본을 사용하도록 모델을 확장하여 RT-1 (Brohan et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 에 대해 제안된 프로토콜을 기반으로 VLA 모델을 인스턴스화한다.

이점: 모델은 웹에서 얻은 지식을 사용하여 이미지와 언어 명령을 해석함으로써 이러한 기술을 새로운 방식으로 사용할 수 있는 능력을 습득한다.

1. 로봇 학습 데이터에 명시적 단서가 없어도, 모델은 기존에 학습한 픽 앤 플레이스 기술을 활용해 숫자나 아이콘 등 의미 있는 기호 근처에 물체를 적절히 배치할 수 있다.
2. 로봇 시연 데이터에 해당 관계가 없더라도, 모델은 객체 간 관계를 스스로 해석해 어떤 물체를 선택하고 어디에 놓을지를 결정할 수 있다.
3. 생각의 사슬을 촉발하는 명령을 했을 경우, 모델은 훨씬 더 복잡한 의미적 추론을 할 수 있다. 예를 들어, 즉석 망치로 사용하기 위해 어떤 물건을 집어야 할지(바위) 또는 피곤한 사람에게 어떤 종류의 음료가 가장 적합할지(에너지 드링크)를 파악할 수 있다.

### 정리

RT-2는 웹 규모 데이터로 훈련된 대규모 시각 언어 모델을 미세 조정하여 일반화 가능하고 의미 인식이 가능한 로봇 정책으로 직접 기능하도록 유도한 모델군이다.

실험은 인터넷 데이터와 이전 연구 (Brohan et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 에서 얻은 명령어 주석이 달린 로봇 궤적을 기반으로 훈련된 최대 550억 개의 매개변수를 가진 모델이다. 

6,000회의 로봇 평가 과정을 통해 RT-2가 객체, 장면 및 명령어에 대한 일반화를 크게 향상시키고, 웹 규모 시각 언어 사전 훈련에서 물려받은 다양한 새로운 기능을 발휘함을 보여준다.

## 2. 관련 작업

### 시각-언어 모델(VLM)

대표적인 2개 유형

(1) 시각과 언어 두 모달리티에 대한 공통 임베딩을 학습하는 표현 학습 모델, 예: CLIP (Radford et al., [2021](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib63))

(2) 시각과 언어를 입력으로 받아 자유 형식의 텍스트를 생성하는 {vision, text} → {text} 형태의 시각-언어 모델

RT-2에서는 후자의 범주에 초점을 맞춘다.

이전 연구들은 로봇 공학을 포함한 다양한 문제 설정에 VLM을 활용해 왔지만, 본 연구의 초점은 VLM의 기능을 로봇 동작 예측으로 확장함으로써 폐루프 제어에까지 활용하는 데 있습니다. 이를 통해 VLM이 이미 보유하고 있는 지식을 이용해 보다 높은 수준의 일반화를 달성하는 방식을 연구함.

### 로봇 학습에서의 일반화

다양한 시나리오에서 광범위하게 성공할 수 있는 로봇 컨트롤러를 개발하는 것은 로봇 연구의 오랜 목표임

이러한 일반화를 가능하게 하는 한 가지 유망한 접근법은 크고 다양한 데이터셋을 활용하여 학습하는 것임

(Pinto and Gupta, [2016](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib60) ; Levine et al., [2018](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib40) ; Dasari et al., [2019](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib13) ) . 

> ... (후략)

그러나 본 연구는 이러한 기존 접근과는 달리, 다양한 조건과 환경에서 일반화가 가능한 **단일 모델(single model)** 의 개발을 목표로 한다. 핵심은 로봇이 직접 관찰하는 데이터에 국한되지 않고, 훨씬 더 방대한 외부 데이터에 노출된 **사전 학습(pretrained)** 모델을 적극 활용하는 데 있다.

### 로봇 조작을 위한 사전 학습

기존 연구는 주로 로봇의 카메라 관찰에 대한 인코더를 초기화하는 데 사용할 수 있는 시각적 표현을 사전 학습하는 것 초점을 맞추고 있음. 
또한 일부 연구는 사전 학습된 언어 모델을 명령어 인코더 또는 고수준 계획 모듈로 통합하기도 했음. 하지만 대부분의 경우, 비전 모델과 언어 모델을 개별적으로 사용했으며, **사전 학습된 시각-언어 모델(VLM)** 을 직접 활용하는 접근은 상대적으로 드물었음

이전 VLM을 사용한 연구는 시각적 상태 표현, 객체 식별, 고수준 계획, 감독이나 성공 감지 제공을 위해 VLM을 사용했음. 

CLIPort (Shridhar et al., [2021](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib68) ) 와 MOO (Stone et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib73) ) 는 사전 훈련된 VLM을 엔드투엔드 방식의 시각-운동 조작 정책에 통합하였으나, 정책 구조에 상당한 제약을 가함으로써 적용성에 제한이 있었음

본 연구는 다음과 같은 점에서 기존 접근과 차별화된다: 

- 제한된 2D 동작 공간에 의존하지 않음
- 보정된 카메라를 필요로 하지 않음
- 언어 생성 기능을 갖춘 VLM을 활용함
- 통합된 출력 공간을 활용하여 언어 및 동작 작업 전반에 걸쳐 **모델 가중치를 완전히 공유** 가능함
- 따라서 **동작 전용 계층** 없이도 동작 및 언어를 모두 처리할 수 있음

## **3. 시각-언어-행동 모델**

VLM을 폐루프 로봇 제어에 직접 활용할 수 있도록 훈련하기 위해, 본 연구는 다음과 같은 모델군과 설계 방안을 제안한다:

1. 일반적인 시각-언어 모델 아키텍처를 설명하고, 이를 로봇 제어에 적용할 수 있는 아키텍처로 변형하는 방법에 대한 설명
2. 웹 규모 데이터로 사전 학습된 대규모 VLM을 미세 조정하여 로봇 동작을 직접 출력하는 **VLA (Vision-Language-Action)** 모델로 전환하는 과정과 그 과제 설명
3. 모델을 로봇 작업에 실용적으로 구현하기 위한 전략을 설명하고, 실시간 제어를 가능하게 하는 모델 크기 및 추론 속도와 관련된 과제를 해결하는 방법 설명

### **3.1 사전 훈련된 시각-언어 모델**

이 연구에서 구축하는 시각 언어 모델 (Chen et al., [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ; Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 은 하나 이상의 이미지를 입력으로 받아 자연어 토큰 시퀀스를 생성하는 모델이다. 이러한 모델은 이미지 구성을 추론하는 것부터 개별 객체와 다른 객체와의 관계에 대한 질문에 답하는 것까지 광범위한 시각적 해석 및 추론 작업을 수행할 수 있다.

이처럼 광범위한 작업을 수행하는 데 필요한 지식을 표현하려면 대규모 모델과 웹 규모 데이터 세트가 필요하다. 본 연구에서는 기존에 제안된 두 가지 VLM, 즉 PaLI-X (Chen et al., [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ) 와 PaLM-E (Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) )를 VLA 모델로 적용하고 있다. 이 두 모델의 시각-언어-행동 버전을 RT-2-PaLI-X와 RT-2-PaLM-E라고 하겠음 (부록 [D](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A4) 에서 이 두 모델의 아키텍처에 대한 자세한 설명을 제공함) 수십억 개에서 수천억 개에 달하는 매개변수를 갖는 이 모델들의 인스턴스들을 활용하였다.


![figure2.png](/assets/img/RT-2/figure2.png)


<그림 2> RT-2는 추론, 기호 이해, 그리고 인간 인식을 필요로 하는 다양한 실제 상황으로 일반화할 수 있다. (Section [4](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4). 에서 구체적인 시나리오들을 제공함)

### **3.2 로봇 액션 미세 조정**

시각 언어 모델이 로봇을 제어할 수 있도록 하려면 동작을 출력하도록 훈련해야함

동작을 모델 출력의 토큰으로 표현하고 이를 언어 토큰과 같은 방식으로 처리합니다. 우리는 RT-1 모델에 대해 Brohan et al.( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 이 제안한 이산화를 기반으로 동작 인코딩을 수행합니다. 동작 공간은 로봇 엔드 이펙터의 6-DoF 위치 및 회전 변위, 로봇 그리퍼의 확장 수준, 성공적인 완료를 알리기 위해 정책에 의해 트리거되어야 하는 에피소드 종료를 위한 특수 이산 명령으로 구성됨

연속 차원(이산 종료 명령을 제외한 모든 차원)은 256개의 빈으로 균일하게 이산화

따라서 로봇 동작은 이산 빈의 순서를 8개의 정수로 사용하여 표현할 수 있음

이러한 이산화된 액션을 사용하여 비전 언어를 비전 언어- *액션* 모델로 미세 조정하려면 모델의 *기존* 토큰화에서 토큰을 이산 액션 빈과 연결해야함

이를 위해 액션 토큰으로 사용할 256개의 토큰을 제공해야함

어떤 토큰을 선택할지는 각 VLM에서 사용하는 특정 토큰화 방식에 따라 달라지며, 이에 대해서는 이 섹션의 뒷부분에서 설명함

VLM 미세 조정 대상을 정의하기 위해 각 차원의 액션 토큰을 공백 문자로 연결하여 액션 벡터를 단일 문자열로 변환함

![output.png](/assets/img/RT-2/outputtoken.png)
ex. "1 128 91 241 5 101 127”

실험에서 미세 조정한 두 개의 VLM인 PaLI-X (Chen et al., [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ) 와 PaLM-E (Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 는 다른 토큰화를 사용함

PaLI-X의 경우 1000까지의 정수는 각각 고유한 토큰을 가지므로 해당 정수를 나타내는 토큰에 액션 빈을 연결하기만 하면됨. 
PaLM-E 모델의 경우 숫자에 대한 표현을 제공하지 않기 때문에 VLM이 기존 토큰을 액션 토큰으로 덮어쓰도록 훈련해야함. 이는 기호 튜닝 (Wei et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib79) ) 의 한 형태이며, 이전 작업에서 VLM에 잘 작동하는 것을 확인했음

위에서 설명한 동작 표현을 사용하여 로봇 데이터를 VLM 모델 미세 조정에 적합하도록 변환
입력에는 로봇 카메라 이미지와 텍스트 작업 설명(표준 VQA 형식인 "Q: 로봇은 [작업 지시]에 대해 어떤 동작을 취해야 합니까? A:")이 포함되고 출력은 로봇 동작을 나타내는 숫자/가장 자주 사용되지 않는 토큰의 문자열 형식

### co-fine tuning

핵심은 로봇 데이터에만 순수 미세 조정을 적용하지 않고 로봇 데이터를 원본 웹 데이터와 공동 미세 조정하는 것임

공동 미세 조정은 로봇 동작뿐만 아니라 웹 스케일 데이터의 추상적인 시각적 개념과 미세 조정 과정에서 저수준 로봇 동작에 모두 노출되기 때문에 더욱 일반화된 정책을 도출할 수 있음

공동 미세 조정 과정에서 로봇 데이터셋의 샘플링 가중치를 높여 각 훈련 배치에서 로봇 데이터와 웹 데이터의 비율을 균형 있게 조정할 수 있음

### 출력 제한

RT-2와 표준 VLM의 중요한 차이점 중 하나는 RT-2가 실제 로봇에서 실행하기 위해 유효한 동작 토큰을 출력해야함.

따라서 디코딩 과정에서 RT-2가 유효한 동작 토큰을 출력하도록 하기 위해, 로봇 동작 작업이 모델에 요청될 때 유효한 동작 토큰만 샘플링하여 출력 어휘를 제한함. (반면, 표준 시각 언어 작업에서는 모델이 모든 자연어 토큰을 출력할 수 있도록 함)

### 3.3 실시간 추론

현대 VLM의 크기는 수십억 또는 수천억 개의 매개변수를 가지고 있음. 이 작업에서 학습된 가장 큰 모델은 55B 개의 매개변수를 사용함. 이러한 모델을 실시간 로봇 제어에 일반적으로 사용되는 표준 데스크톱 스타일 머신이나 로봇 GPU에서 직접 실행하는 것은 불가능함.

직접 폐쇄 루프 로봇 제어에 사용되는 모델 중 규모가 10배 이상 크므로 효율적인 실시간 추론을 가능하게 하는 새로운 솔루션 세트가 필요함. 따라서 다중 TPU 클라우드 서비스에 배포하고 네트워크를 통해 이 서비스를 쿼리하여 로봇에서 RT-2 모델을 실행할 수 있는 프로토콜을 개발해서 사용함

이 솔루션을 통해 적절한 제어 빈도를 달성하고 동일한 클라우드 서비스를 사용하여 여러 로봇을 지원할 수 있음. 우리가 평가한 가장 큰 모델인 55B 매개변수 RT-2-PaLI-X-55B 모델은 1~3Hz의 주파수로 작동할 수 있음 5B 매개변수로 구성된 이 모델의 더 작은 버전은 약 5Hz의 주파수로 작동할 수 있음

## 4. 실험

RT-2는 현실 세계의 일반화 능력과 새로운 역량에 초점을 맞추고, 다음 질문에 답하는 것을 목표로 한다.

1. 눈에 보이는 작업에서 어떤 성능을 보이며, 더 중요하게는 새로운 객체, 배경, 환경에서 일반화하는가?
2. 새로운 역량을 관찰하고 측정할 수 있나?
3. 일반화는 매개변수 수와 기타 설계 결정에 따라 어떻게 달라지는가?
4. 시각-언어 모델과 유사하게 사고의 연속적 추론 능력을 보일 수 있는가?

평가: 우리는 다양한 조건에서 약 6,000개의 평가 궤적을 사용하여 우리의 접근 방식과 여러 기준선을 평가하였다. 7DoF 모바일 매니퓰레이터를 사용하였다. 프로젝트 웹사이트 [robotics-transformer2.github.io](https://robotics-transformer2.github.io/) 에서 RT-2 실행의 예를 제공하고 있다. 사전 훈련된 VLM을 활용하는 RT-2의 두 가지 특정 인스턴스를 훈련합니다.(1) **RT-2-PaLI-X는** 5B 및 55B PaLI-X (Chen et al., [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ) 에서 구축되고, (2) **RT-2-PaLM-E는** 12B PaLM-E (Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 에서 구축되었다. 

훈련에는 Chen et al.( [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ) 및 Driess et al.( [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 의 웹 규모 데이터를 활용하였다. 

이 데이터는 시각적 질문 답변, 캡션, 구조화되지 않은 얽힌 이미지와 텍스트 예제로 구성된다. 

이것을 사무실 주방 환경에서 17개월 동안 13개의 로봇으로 수집한 Brohan et al.( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 의 로봇 시연 데이터와 결합한다. 

각 로봇 시연 궤적에는 수행된 작업을 설명하는 자연어 명령이 주석으로 달려 있으며, 이는 기술을 설명하는 동사(예: "집다", "열다", "넣다")와 조작된 객체를 설명하는 하나 이상의 명사(예: "7up 캔", "서랍", "냅킨")로 구성된다. (사용된 데이터 세트에 대한 자세한 내용은 부록 [B](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A2) 참조)

모든 RT-2 학습 실행에는 학습률 스케줄과 정규화를 포함하여 PaLI-X (Chen et al., [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ) 및 PaLM-E (Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 논문의 하이퍼파라미터를 채택함 (세한 학습 내용은 부록 [E](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A5) 확인)

### **Baseline**

이전 SOTA: 35M 트랜스포머 기반 모델인 **RT-1** (Brohan et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 을 사용

SOTA pre-trained model:  **VC-1** (Majumdar et al., [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib51) ) 과 **R3M** (Nair et al., [2022b](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib57) )을 사용, 정책은 RT-1 백본을 학습하여 표현을 입력으로 받음

VLM을 사용하기 위한 다른 아키텍처와 비교: VLM을 사용하여 의미 맵에 대한 추가 이미지 채널을 만든 다음 RT-1 백본에 공급하는 **MOO** (Stone et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib73) ) 를 사용

(자세한 내용은 부록 [C](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A3) 에 제공)

### **4.1 seen tasks perform, generalize over new objects, backgrounds, and environments**

![figure3.png](/assets/img/RT-2/figure3.png)

<그림 3> 그림 [4](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F4) 및 [7(b)](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F7.sf2) 와 표 [3](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A8.T3) 및 [5](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A8.T5) 의 평가에 사용된 일반화 시나리오의 예시

분포 내 성능과 일반화 역량을 평가하기 위해 RT-2-PaLI-X 및 RT-2-PaLM-E 모델을 이전 섹션에 나열된 4개의 기준선과 비교

아는 작업의 범주의 경우 RT-1 (Brohan et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 과 동일한 보이는 지침 모음을 사용

이 평가에는 200개 이상의 작업이 포함됨. 물건을 집는 작업 36개, 물건을 두드리는 작업 35개, 물건을 똑바로 세우는 작업 35개, 물건을 옮기는 작업 48개, 다양한 서랍을 열고 닫는 작업 18개, 서랍에서 물건을 집어 올리고 넣는 작업 36개.

그러나 이러한 "분포 내" 평가는 여전히 물체의 배치와 시간대 및 로봇 위치와 같은 요소가 다르므로 환경의 현실적인 변동성을 일반화하는 기술이 필요함

Unseen 사물의 경우, 어려운 경우(hard case)에는 파악하기 어렵고 독특한 사물(예: 장난감)이 포함 
Unseen 배경의 경우, 어려운 경우(hard case)에는 더욱 다양한 배경과 새로운 사물이 포함됨
Unseen 환경의 경우, 어려운 경우(hard case)는 모니터와 액세서리가 있는 시각적으로 더 뚜렷한 사무실 책상 환경에 해당하고, 쉬운 환경은 주방 싱크대임.

평가는 다양한 시나리오에서 픽 앤 플레이스(pick and placement) 기술에 주로 초점을 맞춘 280개 이상의 과제로 구성되며, Unseen 에 대한 지침 목록은 부록 [F.2](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A6.SS2) 에 명시되어있음

![figure4.png](/assets/img/RT-2/figure4.png)

<그림 4> 새로운 객체, 새로운 배경, 새로운 환경에 대한 일반화를 측정함. Seen과 Unseen의 평가에 대한 베이스라인과의 비교. 전체 결과는 부록 표 [3](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A8.T3) 에서 확인

RT-2 모델과 RT-1 간의 보이는 작업에 대한 성능은 비슷했으며 다른 기준선은 낮은 성공률을 달성

RT-2 모델과 기준선 간의 차이는 다양한 일반화 실험에서 가장 두드러졌으며, 이는 시각-언어-행동 모델의 강점이 인터넷 규모의 사전 학습 데이터에서 더 일반화 가능한 시각적 및 의미적 개념을 전송하는 데 있음을 시사함

평균적으로 RT-2의 두 인스턴스화는 비슷한 성능을 보였으며, 다음 두 기준선인 RT-1과 MOO보다∼2배, 다른 기준선보다 ∼6배 더 좋았음

RT-2의 PaLM-E 버전은 일반화 시나리오의 더 어려운 버전에서는 RT-2-PaLI-X보다 더 나은 성능을 보였지만 쉬운 버전에서는 성능이 떨어져 비슷한 평균 성능을 보임

**오픈 소스 언어 테이블 벤치마크**

오픈 소스 기준선과 환경을 사용하여 추가 비교 지점을 제공하기 위해 Lynch et al.( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib47) ) 의 오픈 소스 언어 테이블 시뮬레이션 환경을 활용함.

언어 테이블 데이터 세트에 대해 도메인 내 VQA 작업을 포함한 여러 예측 작업에서 더 작은 PaLI 3B 모델을 공동 미세 조정하고 시뮬레이션에서 결과 정책을 평가함

동작 예측 작업의 경우 동작을 " XY " 형식의 텍스트로 이산화하고 인코딩함.

X 와 Y는 {-10, -9, …, +9, +10} 사이이며 엔드 이펙터는 2D 좌표계의 점 변화율(delta 2D cartesian setpoints)을 나타냄

크기가 줄어들었기 때문에 결과 모델은 다른 기준선과 비슷한 속도(5Hz)로 추론을 실행할 수 있음

실험의 결과는 표 [6](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F6) 에서 확인

기준선과 비교했을 때 본 모델을 사용했을 때 상당한 성능 향상이 이루어짐, 이는 VLM 기반 사전 학습과 대규모 PaLI 모델의 표현력이 다른 시나리오, 특히 다른 로봇을 이용한 시뮬레이션에도 유용할 수 있음을 시사

그림 [6](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F6) 에서는 실제 환경에서 분포 외 행동의 정성적인 결과를 보여주며, 이 환경에서는 이전에 볼 수 없었던 새로운 밀기 작업과 목표 물체 조준 기능을 보여줌. 언어 테이블 실험에 대한 자세한 내용은 부록 [B](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A2) 와 [D](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A4) 에서 확인

![figure5.png](/assets/img/RT-2/figure5.png)

<그림 5> 언어-테이블 환경에서의 실제 분포 외 행동. 표 [6](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F6) 과 동일한 RT-2-PaLI-3B 모델 체크포인트가 사용

![figure6.png](/assets/img/RT-2/figure6.png)

<그림 6> 시뮬레이션된 언어-테이블 작업의 성능 (Lynch 및 Sermanet, 2020)

### **4.2 RT-2의 새로운 역량을 관찰하고 측정할 수 있는가**

시각-언어-행동 모델의 일반화 능력을 평가하는 것 외에도, 이러한 모델이 웹에서 얻은 지식을 전이함으로써 로봇 데이터에서 입증된 것 이상의 새로운 능력을 어느 정도까지 구현할 수 있는지 평가하고 싶음. 이러한 능력을 인터넷 규모의 사전 학습을 전이함으로써 새롭게 나타난다는 의미에서 *'창발적'* 이라고 부름

이러한 전이가 새로운 로봇 *동작을* 가능하게 할 것이라고 기대하지는 않지만, 관계와 명사를 포함한 의미론적 및 시각적 개념이 로봇 데이터에서 발견되지 않은 경우에도 효과적으로 전이될 것으로 기대함

### **정성적 평가**

RT-2-PaLI-X 모델을 사용하여 시각-언어 개념에서 전달된 다양한 창발적 역량

"딸기를 올바른 그릇에 담기"라는 작업을 수행하려면 딸기와 그릇이 무엇인지에 대한 미묘한 이해뿐만 아니라 딸기가 비슷한 과일과 함께 있어야 한다는 것을 장면 맥락에서 추론해야함. "테이블에서 떨어지려는 가방을 집어 올리기" 작업의 경우 RT-2는 두 가방을 구별하고 위태롭게 놓인 물체를 인식하는 물리적 이해를 보여줌. 이러한 시나리오에서 테스트된 모든 상호 작용은 로봇 데이터에서 한 번도 본 적이 없었으며, 이는 시각-언어 데이터에서 의미적 지식이 전달되었음을 나타냄

### **정량적 평가**

새로운 역량을 정량화하기 위해 이전 평가에서 상위 두 기준선인 RT-1과 VC-1을 선택하여 두 모델인 RT-2-PaLI-X와 RT-2-PaLM-E와 비교함

실험의 분산을 줄이기 위해 A/B 테스트 프레임워크 (Fisher, [1936](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib20) ) 를 사용함

RT-2의 새로운 역량을 추론과 의미 이해의 축을 포함하는 세 가지 범주로 나눴음 (각각의 예는 부록 그림 [9](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A6.F9) 에서 확인)

1. 기호 이해: RT-2 정책이 로봇 데이터에 없는 시각 언어 사전 학습에서 의미 지식을 전달하는지 여부를 명시적으로 테스트함.이 범주의 예시 지침은 "사과를 3으로 옮기기" 또는 "콜라 캔을 하트 위로 밀어 올리기”임
2. 추론: 기본 VLM의 추론의 다양한 측면을 제어 작업에 적용하는 능력. 이러한 작업에는 시각적 추론("사과를 같은 색상의 컵으로 옮기기"), 수학("X를 2 더하기 1의 합 근처로 옮기기") 및 다국어 이해("mueve la manzana al vaso verde")가 필요함.
3. 인간 인식 작업: 인간 중심적 이해와 인식. "콜라 캔을 안경을 쓴 사람에게 옮기기"와 같은 작업이 포함

이 평가에 사용된 지침의 전체 목록은 부록 [F.2](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A6.SS2) 에 명시했음

그림 [7(a)](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F7.sf1) 에 이 실험의 결과를 제시하고 모든 수치 결과는 부록 [H.2](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A8.SS2) 에 있음

VLA 모델이 모든 범주에서 기준선보다 상당히 우수한 성능을 보이며, 가장 우수한 RT-2-PaLI-X 모델이 두 번째로 우수한 기준선(RT-1)보다 평균 성공률이 3배 이상 높은 것을 관찰함

또한 더 큰 PaLI-X 기반 모델이 평균적으로 기호 이해, 추론 및 사람 인식 성능이 더 나은 반면, 더 작은 PaLM-E 기반 모델은 수학적 추론이 포함된 작업에서 우위를 점한다는 점에 주목

이 흥미로운 결과는 PaLM-E에서 사용된 다양한 사전 학습 혼합에 기인하며, 그 결과 대부분 시각적으로 사전 학습된 PaLI-X보다 수학적 계산 능력이 더 뛰어난 모델이 탄생함을 알려줌

![figure7.png](/assets/img/RT-2/figure7.png)

<그림 7> ( [7(a)](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F7.sf1) )Performance comparison on various emergent skill evaluations (Figure [9](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A6.F9)) between RT-2 and two baselines.
( [7(b)](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F7.sf2) ) Ablations of RT-2-PaLI-X showcasing the impact of parameter count and training strategy on generalization.  

부록 표 [4](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A8.T4) 와 [5는](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A8.T5) 전체 수치 결과를 자세히 설명함


### **4.3 일반화는 매개변수 수와 기타 설계 결정에 따라 어떻게 달라지나요?**

이 비교를 위해, 모델 크기 측면에서 유연성이 높은 RT-2-PaLI-X 모델을 사용(PaLM-E의 특성상 RT-2-PaLM-E는 특정 크기의 PaLM 및 ViT 모델로만 제한됨)

특히, 두 가지 서로 다른 모델 크기인 5B와 55B, 그리고 세 가지 서로 다른 학습 루틴을 비교함

VLM 사전 학습의 가중치를 사용하지 않고 처음부터 모델을 학습하는 방법, 로봇 동작 데이터만 사용하여 사전 학습된 모델을 미세 조정하는 방법, 그리고 본 연구에서 주로 사용하는 방법인 공동 미세 조정(미세 조정과 공동 학습)은 VLM 미세 조정을 위해 원본 VLM 학습 데이터와 로봇 데이터를 모두 사용함

모델의 일반화 측면에 주로 관심이 있으므로, 본 실험 세트에서는 *관찰된 작업* 평가는 제외함

Ablation 결과는 그림 [7(b)](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F7.sf2) 와 부록 표 [5](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A8.T5) 에 제시

1. 매우 큰 모델을 처음부터 학습하면 5B 모델에서도 매우 낮은 성능을 보임 이 결과를 바탕으로, 처음부터 학습한 훨씬 더 큰 55B PaLI-X 모델의 평가는 건너뛰기로 결정함
2. 모델의 크기에 관계없이 공동 미세 조정이 로봇 데이터로 미세 조정하는 것보다 일반화 성능이 더 우수함 이는 미세 조정 부분에서 원본 데이터를 유지하면 모델이 VLM 학습 중에 학습한 이전 개념을 잊지 않기 때문임. 
3. 모델의 크기가 커질수록 일반화 성능이 더 우수함

### **4.4 RT-2는 시각-언어 모델과 유사하게 연속적 추론이 가능할까?**

LLM의 사고의 사슬 촉진 방법 (Wei et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib78) ) 에서 영감을 얻어, 언어와 행동을 함께 활용하는 능력을 높이기 위해 PaLM-E를 사용하여 RT-2의 변형을 수백 개의 기울기 단계로 미세 조정하여 더 정교한 추론 행동을 이끌어낼 수 있기를 바람

로봇이 자연어로 취하려는 행동의 목적을 먼저 설명하는 추가 "계획" 단계를 포함하도록 데이터를 확장한 다음 실제 행동 토큰(예: "지시: 배고파요. 계획: rxbar 초콜릿을 고르세요. 행동: 1 128 124 136 121 158 111 255")을 추가함

이 데이터 확장 체계는 VQA 데이터 세트(시각적 추론)와 조작 데이터 세트(행동 생성) 간의 브릿지 역할을 함

우리는 사고 연쇄 추론을 적용한 RT-2가 자연어로 먼저 동작을 계획할 수 있는 공간을 제공받기 때문에 더욱 정교한 명령에 응답할 수 있음을 정성적으로 관찰함

이는 LLM 또는 VLM을 계획자로 사용하는 것 (Ahn et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib1) ; Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 이 단일 VLA 모델에서 저수준 정책과 결합될 수 있다는 초기 증거를 제공한다고 생각함

사고 연쇄 추론을 적용한 RT-2의 롤아웃(?)은 그림 [8](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F8) 과 부록 [I](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A9) 에 나와 있음

![figure8.png](/assets/img/RT-2/figure8.png)

<그림 8> 사고의 사슬 추론을 통한 RT-2의 롤아웃. 여기서 RT-2는 계획과 작업을 모두 생성한다.

## 5. 한계

1. VLM을 통한 웹 스케일 사전 학습을 포함하면 의미 및 시각적 개념에 대한 일반화가 향상됨을 보여주지만, 로봇은 이러한 추가 경험을 포함함으로써 새로운 *동작을* 수행하는 능력을 습득하지 못함
모델의 물리적 기술은 여전히 로봇 데이터에서 확인된 기술 분포에 국한되지만(부록 [G](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A7) 참조), 새로운 방식으로 이러한 기술을 활용하는 방법을 학습함. 데이터 세트가 충분히 다양하지 않기 때문이라고 생각함. 
향후 연구의 흥미로운 방향은 인간 비디오와 같은 새로운 데이터 수집 패러다임을 통해 새로운 기술을 어떻게 습득할 수 있는지 연구하는 것!
2. 대규모 VLA 모델을 실시간으로 실행할 수 있음을 보여주었지만, 이러한 모델의 계산 비용이 높고, 이러한 방법이 고주파 제어가 필요한 설정에 적용됨에 따라 실시간 추론이 주요 병목 현상이 될 수 있음
향후 연구의 흥미로운 방향은 이러한 모델을 더 높은 속도 또는 더 저렴한 하드웨어에서 실행할 수 있도록 하는 양자화 및 증류 기술을 탐구하는 것!!
RT-2를 생성하는 데 사용할 수 있는 일반적으로 사용 가능한 VLM 모델의 수가 적다는 또 다른 현재 한계와도 관련됨. 더 많은 오픈 소스 모델(예: https://llava-vl.github.io/ )이 제공되고, 독점 모델이 미세 조정 API를 공개하여 VLA 모델을 구축하는 데 충분한 요건을 충족하기를 바람

## 6. 결론

본 논문에서는 비전-언어-행동(VLA) 모델을 비전-언어 모델(VLM) 사전 학습과 로봇 데이터를 결합하여 학습하는 방법을 설명함. PaLM-E와 PaLI-X를 기반으로 한 두 가지 VLA 인스턴스(RT-2-PaLM-E와 RT-2-PaLI-X)를 제시함. 이 모델들은 로봇 궤적 데이터와 함께 미세 조정되어 로봇 동작을 출력하며, 로봇 동작은 텍스트 토큰으로 표현됨. 

# 부록

## 부록 A. 기여

(생략)

## 부록 B. 데이터셋

시각-언어 데이터 세트는 Chen et al.( [2023b](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib9) ) 및 Driess et al.( [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 의 데이터 세트 혼합을 기반으로 함

이 데이터셋은 109개 언어에 걸쳐 약 100억 개의 이미지-텍스트 쌍인 WebLI 데이터 세트로 구성되며, 상위 10%의 점수를 받은 교차 모달 유사성 예제로 필터링하여 10억 개의 훈련 예제를 제공함. 

RT-2-PaLI-X의 경우 Chen et al.( [2023b](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib9) ) 에서, RT-2-PaLM-E의 경우 Driess et al.( [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 에서 데이터 세트 혼합에 대한 자세한 정보를 찾을 수 있음

RT-2-PaLI-X를 공동 미세 조정할 때 Chen et al.( [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ) 에서 설명한 Episodic WebLI 데이터 세트를 사용하지 않았음

로봇 데이터셋은 Brohan 외( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 의 데이터셋을 기반으로 함. 

이 데이터셋은 모바일 조작 로봇을 통해 수집된 시연 에피소드들로 구성되어 있습니다. 각 시연에는 " 물체 집기", " 물체 근처로 물체 옮기기", "물체를 똑바로 세우기", " 물체를 넘어뜨리기", " 서랍 열기", " 서랍 닫기", " 물체를 용기 에 넣기", " 용기 에서 물체 를 집어 카운터에 놓기"의 7가지 기술 중 하나에 대한 자연어 명령 이 주석으로 첨부되어 있음.

자세한 내용은 Brohan 외( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 에서 확인

RT-2-PaLI-X는 로봇 데이터셋에 가중치를 부여하여 공동 미세 조정을 위한 훈련 mixture의 약 50%를 차지하도록 했음. T-2-PaLM-E는 로봇 데이터셋에 가중치를 부여하여 훈련 혼합물의 약 66%를 차지하도록 함

표 [6](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F6) 의 Language-Table 결과에 대해, 우리 모델은 Lynch et al.( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib47) ) 의 Language-Table 데이터세트에서 학습한다. 여러 예측 작업에 대해 공동 미세 조정했음

(1) 두 개의 연속된 이미지 프레임과 텍스트 명령이 주어졌을 때 동작을 예측
(2) 이미지 프레임이 주어졌을 때 명령을 예측
(3) 이미지 프레임이 주어졌을 때 로봇 팔 위치를 예측 
(4) 주어진 이미지 프레임 사이의 타임스텝 수를 예측 
(5) 이미지 프레임과 명령이 주어졌을 때 작업이 성공했는지 여부를 예측

## 부록 C. 베이스라인

**RT-1** : 로보틱스 트랜스포머 1 Brohan 외( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib4) ) 는 트랜스포머 기반 모델로, 발표 당시 유사한 작업군에서 최고 성능을 달성. 이 모델은 VLM 기반 사전 학습을 사용하지 않으므로, VLM 기반 사전 학습의 중요성을 보여주는 중요한 데이터 포인트를 제공함

**VC-1** : VC-1 Majumdar et al.( [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib51) ) 은 로봇 작업을 위해 특별히 설계된 사전 훈련된 시각적 표현을 사용하는 시각적 기반 모델임. VC-1 ViT-L 모델의 사전 훈련된 표현을 사용했음. VC-1에는 언어 조절이 포함되지 않으므로 Universal Sentence Encoder Cer et al.( [2018](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib6) ) 을 통해 언어 명령을 별도로 내장하여 우리 방법과 비교할 수 있도록 추가했음. 결과 언어 내장 토큰을 VC-1에서 생성된 이미지 토큰에 연결하고 연결된 토큰 시퀀스를 토큰 학습기 Ryoo et al.( [2021](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib65) )을 통해 전달하도록 했음. 토큰 학습기에서 생성된 토큰 시퀀스는 RT-1 디코더 전용 변환기 모델에서 사용되어 로봇 동작 토큰을 예측함. VC-1 기준선을 종단간으로 훈련하고 훈련 중에 VC-1 가중치를 동결 해제함. 동결된 VC-1 가중치를 사용하는 것보다 훨씬 더 나은 결과를 얻을 수 있었음

**R3M** : R3M Nair et al.( [2022b](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib57) )은 정책 학습을 개선하기 위해 사전 학습된 시각 언어 표현을 사용한다는 점에서 VC-1과 유사한 방법임. 이 경우 저자는 정책에서 사용하는 표현을 학습하기 위해 인간 활동의 Ego4D 데이터 세트 Grauman et al.( [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib24) ) 을 사용함. VC-1과 R3M은 모두 VLM을 사용하는 대안으로 다양한 최첨단 표현 학습 방법을 테스트. R3M 사전 학습된 표현에서 언어 조건 정책을 얻기 위해 위에서 VC-1에 대해 설명한 것과 동일한 절차를 따르지만 R3M ResNet50 모델을 사용하여 이미지 토큰을 얻고 학습 중에 동결을 해제함.

**MOO** : MOO Stone 외( [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib73) ) 는 객체 중심 접근법으로, VLM을 사용하여 원본 이미지에서 단일 색상 픽셀 형태로 관심 객체를 먼저 지정함. 이렇게 픽셀이 수정된 이미지는 일련의 조작 작업을 수행하기 위해 종단 간 정책으로 학습됨. 이 베이스라인은 VLM이 인식을 향상시키는 별도의 모듈로 사용되지만, 그 표현은 정책 학습에 사용되지 않는 상황에 해당함.

## **부록 D RT-2용 VLM**

PaLI-X 모델 아키텍처는 이미지를 처리하는 ViT-22B Dehghani et al.( [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib14) ) 으로 구성되며, n개의 이미지 시퀀스를 허용하여 이미지당 n×k개의 토큰을 생성함. (k는 이미지 당 패치 수)

투영 계층을 통과하는 이미지 토큰은 UL2 Tay et al.( [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib75) ) 과 유사하게 32B개의 매개변수와 50개의 계층으로 구성된 인코더-디코더 백본에서 사용됨

이 백본은 텍스트와 이미지를 임베딩으로 공동 처리하여 자기 회귀 방식으로 출력 토큰을 생성함. 

텍스트 입력은 일반적으로 작업 유형과 추가 컨텍스트(예: 캡션 작업의 경우 "⟨lang⟩로 캡션 생성" 또는 VQA 작업의 경우 "⟨lang⟩으로 답변: 질문")로 구성됨

Language-Table(표 [6](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F6) )에서 학습된 PaLI-3B 모델은 이미지를 처리하기 위해 더 작은 ViT-G/14 (Zhai et al., [2022](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib86) ) (2B 매개변수)를 사용하고 인코더-디코더 네트워크에는 UL2-3B (Tay et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib75) ) 를 사용함

PaLM-E 모델은 이미지 및 텍스트와 같은 로봇 데이터를 언어 토큰 공간에 투영하고 상위 계획과 같은 텍스트를 출력하는 디코더 전용 LLM을 기반으로함. 

사용된 PaLM-E-12B의 경우, 이미지를 언어 임베딩 공간에 투영하는 데 사용된 시각 모델은 ViT-4B Chen et al. ( [2023b](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib9) ) 이다. 텍스트 입력에 연속 변수를 연결하면 PaLM-E는 완전한 다중 모드가 되어 다중 센서 모달리티, 객체 중심 표현, 장면 표현 및 객체 엔티티 참조와 같은 다양한 입력을 수용할 수 있음. 

## **부록 E 교육 세부 정보**

PaLI-X (Chen et al., [2023a](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib8) ) 5B & 55B 모델, PaLI (Chen et al., [2023b](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib9) ) 3B 모델, PaLM-E (Driess et al., [2023](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#bib.bib15) ) 12B 모델에서 사전 학습된 모델에 대해 공동 미세 조정을 수행함. 

RT-2-PaLI-X-55B의 경우, 학습률 1e-3과 배치 크기 2048을 사용하고 80K 그래디언트 단계에 대해 공동 미세 조정을 수행

RT-2-PaLI-X-5B의 경우, 동일한 학습률과 배치 크기를 사용하고 270K 그래디언트 단계에 대해 공동 미세 조정을 수행

RT-2-PaLM-E-12B의 경우, 학습률 4e-4와 배치 크기 512를 사용하여 1M 그래디언트 단계에 대해 공동 미세 조정을 수행

표 [6](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.F6) 의 언어표 결과에 사용된 RT-2-PaLI-3B 모델의 경우, 학습률 1e-3과 배치 크기 128을 사용하여 300K 그래디언트 스텝 동안 모델을 공동 미세 조정

## **부록 F 평가 세부 사항**

### **F.1 평가 시나리오**

![figure9.png](/assets/img/RT-2/figure9.png)

<그림 9> 시나리오 중 일부에 대한 시각적 개요. 이 시나리오들은 (a) 추론, (b) 기호 이해, (c) 인간 인식의 세 가지 범주에 초점을 맞추며, 시각화된 명령어는 부록 [F.2](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A6.SS2) 에 나열된 전체 명령어의 일부임

### **F.2 평가 지침**

표 [1은](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A6.T1) 보이지 않는 객체, 배경 및 환경에 대한 모델 평가에 사용된 자연어 명령어를 나열함.  
각 명령어는 해당 평가 세트의 총 명령어 수에 따라 1회에서 5회까지 실행함.  
표 [2](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#A6.T2) 는 정량적 창발적 평가에 사용된 자연어 명령어를 나열함.  

![figure9.png](/assets/img/RT-2/figure9.png)


## **부록 G 실패 사례 예시**

생략

## **부록 H 정량적 실험 결과**

### **H.1 섹션 [4.1](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.SS1) 의 전반적인 성능**
![figure9.png](/assets/img/RT-2/figure9.png)


### **H.2 Emergent Evaluation, for Section [4.2](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.SS2)**

![figure9.png](/assets/img/RT-2/figure9.png)

### **H.3 Size and Training Ablations, for Section [4.3](https://ar5iv.labs.arxiv.org/html/2307.15818?_immersive_translate_auto_translate=1#S4.SS3)**

![figure9.png](/assets/img/RT-2/figure9.png)

## **부록 I 추가적인 Chain-Of-Thought 추론 결과**

![figure11.png](/assets/img/RT-2/figure11.png)


