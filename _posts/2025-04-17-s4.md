---
title: "The Annotated S4 이해하기 ①: 상태공간방정식부터 S4의 구현까지"
date: 2025-04-17 23:00:00 +0900
description: "본 글은 S4에 대한 이해를 돕기 위해 깃허브 문서(Annotated-S4)를 바탕으로 작성된 글입니다."
math: true
image:
  path: /assets/img/s4/2025-04-17-thumb.jpg
  alt: "S4"
categories: [SSM, S4, Sequence Modeling]
tags: [SSM, S4, Annotated-S4, LRA]
---

> 본 글은 Annotated-S4([링크](https://srush.github.io/annotated-s4/))를 바탕으로 상태공간 모델을 활용한 딥러닝 모델 S4의 기초를 정리한 내용입니다.  
> 모든 내용은 [Annotated-S4 Github 문서](https://srush.github.io/annotated-s4/)와 [그에 대한 코드](https://github.com/srush/annotated-s4)에 기반하고 있습니다. 
> SSM에 대한 기본적인 이해가 있다고 가정합니다.

---

## 상태공간방정식과 상태천이행렬

상태공간방정식은 시간에 따라 변화하는 시스템의 동작을 수식으로 표현한 것으로, 현재 상태 $x$와 입력 $u$를 이용해 다음 상태를 계산하고, 그 상태와 입력으로부터 출력 $y$를 구하는 연립방정식입니다.

$$
\begin{aligned}
\dot{x}(t) &= Ax(t) + Bu(t) \\
y(t) &= Cx(t) + Du(t)
\end{aligned}
$$

이 시스템의 동적 특성을 결정하는 것은 $A$ 행렬로, 이 행렬을 바탕으로 상태천이행렬(state transition matrix) $\Phi(t)$를 정의합니다.

$$
\Phi(t) = e^{At}
$$

입력이 없을 경우($u(t) = 0$) 상태는 다음과 같이 표현됩니다:

$$
x(t) = \Phi(t)x(0) = e^{At}x(0)
$$

초기 상태 $x(0)$로부터 출발한 시스템은, $A$ 행렬에 의해 정의되는 동역학에 따라 시간 $t$ 후의 상태 $x(t)$가 결정됩니다.
이는 시스템의 **자연 응답(natural response)**을 나타냅니다.
만약 외부 제어 입력 $u(t)$가 존재한다면, 전체 해는 다음과 같이 표현됩니다:

$$
x(t) = e^{At}x(0) + \int_0^t e^{A(t-\tau)}Bu(\tau)\, d\tau
$$


> 이 지수 행렬은 다음과 같이 테일러 급수로 정의됩니다.

$$
e^{At} = I + At + \frac{1}{2!}A^2t^2 + \cdots
$$

> Q. 행렬 A를 지수에 올릴 수 있다고? 그것 또한 행렬이라고?  
> A. 참고 영상: [3Blue1Brown - How (and why) to raise e to the power of a matrix](https://www.youtube.com/watch?v=DE6xU2cE3iA)

> 주의사항
> - 시스템이 시불변일 경우에만 $Φ(t) = e^{At}$로 표현 가능합니다. 
> - $A$를 transition matrix라고도 하는데, 시스템 행렬 $A$와 상태 천이 행렬 $Φ$는 다른 것입니다. 


---

## A, B, C 랜덤 초기화 및 이산화

학습의 목표는 $A, B, C, D$를 경사하강법으로 학습하는 것입니다. 보통 $D$는 skip connection으로 생략합니다. 즉, 출력 $y$는 상태 $x$로만 결정되도록 단순화합니다.
> Q. Skip connection이라서 생략한다는 게 무슨 말인지?  
> A. $D$는 입력이 출력에 **즉시 반영되는 경로(skip connection)**를 의미하지만, 대부분의 경우 상태 $x$가 입력 정보를 충분히 담고 있다고 가정하므로 모델을 단순화하기 위해 **$D$를 생략(D = 0)**한다고 이해합시다.

- $A \in \mathbb{R}^{N \times N}$
- $B \in \mathbb{R}^{N \times 1}$
- $C \in \mathbb{R}^{1 \times N}$

> Q. 왜 이산화(discretization)를 해야하는가?  
> A. 위의 방정식은 연속시간 상태공간 방정식이므로, 실제 계산에서는 이산시간 상태공간 방정식으로 바꿔야 합니다. 수치적 적분법을 이용하여 다음 상태를 근사해야 하며, 대표적인 방법은 다음과 같습니다:
> - Forward Euler Method
> - Backward Euler Method
> - **Bilinear Method (Tustin Method)**
> - ZOH 
> - Runge-Kutta Method

논문에서는 Bilinear Method를 채택합니다. 이산화된 시스템의 행렬 파라미터 $A$, $B$, $C$는 $\bar{A}$, $\bar{B}$, $\bar{C}$ 로 표기합니다.

$$
\bar{A} = \left(I - \frac{\Delta}{2} A \right)^{-1} \left(I + \frac{\Delta}{2} A \right)
$$

$$
\bar{B} = \left(I - \frac{\Delta}{2} A \right)^{-1} \Delta B
$$

$$
\bar{C} = C
$$

> 왜 이렇게 되는지는 심화편에서 설명하겠습니다.

다시 정리하면
- 연속시간 시스템은 상태의 변화율인 $\dot{x}(t)$를 정의함
- 실제 계산에서는 이산시간 $x(t + \Delta t)$로 추정해야 함
- 따라서 수치적 적분 방법(Euler, Bilinear 등)이 필요함

참고:

- [Bilinear Transform - Wikipedia](https://en.wikipedia.org/wiki/Bilinear_transform)
- [[Tistory] 연속시간 상태공간방정식의 이산화(ZOH)](https://pasus.tistory.com/321)

---

## 이산시간 SSM은 RNN과 닮았다

> Discrete SSM is RNN-like

이산화한 SSM은 다음과 같은 형태를 갖습니다:

$$
x_k = \bar{A} x_{k-1} + \bar{B} u_k
$$

$$
y_k = \bar{C} x_k
$$

>이산화된 시스템에서는 연속시간 변수 $t$ 대신 이산 시간 인덱스 $k$를 사용하여 표현합니다. 
> 하지만  $t$로 적혀있어도 인덱스 형식이면 이산시간 인덱스로 해석합시다.

function-to-function인 연속시간 SSM과 달리 이산시간 SSM은 sequence-to-sequence 입니다. 이 구조는 다음과 같은 전통적 RNN의 계산과 유사한 구조를 가집니다:

$$
h_t = \tanh(W_h h_{t-1} + W_x x_t)
$$

$$
y_t = W_y h_t
$$

> 입력과 상태를 나타내는 기호는 분야에 따라 다르게 사용될 수 있습니다.
예를 들어, 제어이론에서는 보통 입력을 $u$, 상태를 $x$로 표기합니다. 반면, 딥러닝에서는 입력을 $x$로 쓰는 경우가 많아 입력을 $x$, 상태를 $h$로 표기합니다.
이처럼 사용자에 따라 표기법이 다르기 때문에, 문맥을 보고 각 기호가 무엇을 의미하는지 혼동하지 않도록 주의해야 합니다.

JAX의 lax.scan을 사용하면 RNN-like한 SSM 계산을 빠르고 효율적으로 구현할 수 있습니다. 


```python
def scan_SSM(Ab, Bb, Cb, u, x0):
    def step(x_k_1, u_k):
        x_k = Ab @ x_k_1 + Bb @ u_k
        y_k = Cb @ x_k
        return x_k, y_k

    return jax.lax.scan(step, x0, u)
```

초기 상태 $x_0$와 step size ${\Delta}$를 고정하고, 입력 토큰 $u_k$와 현재 상태 $x_{k-1}$를 이용해 다음 상태 $x_k$와 출력 $y_k$를 계산하는 **1-loop 계산 함수 step**를 정의합니다. 

즉, 한 시점의 상태와 출력을 계산하는 step을 만들고, jax.lax.scan으로 시퀀스 전체에 걸쳐 step을 반복 계산합니다. jax.lax.scan은 일종의 고속 for-loop 함수로, 반복 연산을 효율적으로 수행하면서 JAX의 JIT 컴파일과 자동 미분이 제대로 작동하도록 최적화된 구조입니다. 

```python
def run_SSM(A, B, C, u):
    L = u.shape[0]
    N = A.shape[0]
    Ab, Bb, Cb = discretize(A, B, C, step=1.0 / L)

    # Run recurrence
    return scan_SSM(Ab, Bb, Cb, u[:, np.newaxis], np.zeros((N,)))[1]
```
jax.lax.scan으로 한 스텝 계산 함수 step을 감싸면, 단순히 입력 시퀀스 $u$를 함수에 전달하는 것만으로 출력 시퀀스 $y$ 전체를 얻을 수 있습니다.

이때 run_SSM 함수는 입력 시퀀스 $u$를 받아서 시퀀스 길이 $L$에 해당하는 모든 상태 $x_k$와 출력 $y_k$를 순차적으로 계산하여 반환합니다. 

즉, 길이 $L$의 입력 벡터를 넣으면, 동일한 길이의 출력 벡터 $y$가 반환되는 구조입니다.

> Tangent: A Mechanics Example
잘 정의된 시스템 행렬 $A, B, C$가 주어진 경우, 입력 시퀀스 $u$에 대해 출력 $y$가 어떻게 계산되는지를 구현할 수 있습니다. 설명은 생략합니다.

---
## SSM의 훈련은 CNN과 닮았다

RNN-like 구조는 토큰을 순차적으로 처리해야 하기 때문에 학습 효율이 낮은 편입니다. 반면, 컨볼루션 구조에서는 미리 계산된 **커널(K)**을 이용해 전체 출력 시퀀스를 한 번에 계산할 수 있어 효율성이 크게 향상됩니다.

마침 이산 시간 상태공간 모델(SSM)은 unrolling을 통해 다음과 같은 형태로 표현할 수 있습니다:

$$
\begin{aligned}
x_0 &= \bar{B} u_0 \\
x_1 &= \bar{A} \bar{B} u_0 + \bar{B} u_1 \\
x_2 &= \bar{A}^2 \bar{B} u_0 + \bar{A} \bar{B} u_1 + \bar{B} u_2 \\
&\;\vdots \\
y_0 &= \bar{C} \bar{B} u_0 \\
y_1 &= \bar{C} \bar{A} \bar{B} u_0 + \bar{C} \bar{B} u_1 \\
y_2 &= \bar{C} \bar{A}^2 \bar{B} u_0 + \bar{C} \bar{A} \bar{B} u_1 + \bar{C} \bar{B} u_2 \\
&\;\vdots
\end{aligned}
$$

$$
y_k = \bar{C} \bar{A}^k \bar{B} u_0 + \bar{C} \bar{A}^{k-1} \bar{B} u_1 + \cdots + \bar{C} \bar{A} \bar{B} u_{k-1} + \bar{C} \bar{B} u_k
$$

위와 같은 구조는 커널을 정의함으로써 컨볼루션 형태로 벡터화할 수 있습니다:

$$
\bar{K} \in \mathbb{R}^L = \left( \bar{C} \bar{B},\; \bar{C} \bar{A} \bar{B},\; \ldots,\; \bar{C} \bar{A}^{L-1} \bar{B} \right)
$$

커널 $\bar{K}$는 시퀀스 길이 $L$에 대응하는 필터로, 다음과 같이 입력 시퀀스 $u$와 컨볼루션 연산을 수행하면 **전체 출력 시퀀스 $y$를 한 번에 계산**할 수 있습니다:

$$
y = \bar{K} * u
$$


>이때 $\bar{K}$와 $u$는 단순히 element-wise 곱을 하는 것이 아니라, 컨볼루션 정의에 따라 $\bar{K}$를 슬라이딩하면서 또 다른 벡터(u)의 구간과 내적을 반복하는 구조입니다. 
> 컨볼루션은 원래 커널을 반전시켜 슬라이딩하며 입력과 내적을 계산하는 연산이지만, 실제 구현에서는 커널 $\bar{K}$의 인덱스 방향을 어떻게 정의하느냐에 따라 반전이 생략될 수 있습니다. 

컨볼루션 연산은 연산량이 많기 때문에 컨볼루션을 직접 계산하지 않고 Fast Fourier Transform (FFT)을 사용하여 빠르게 계산하는 방법을 사용합니다. 

**이산 컨볼루션 정리(discrete convolution theorem)**는 두 시퀀스 간의 순환 컨볼루션(circular convolution)에 대해, 각 시퀀스의 FFT를 곱한 뒤 역 FFT(Inverse FFT)를 적용하면 컨볼루션 결과를 효율적으로 계산할 수 있음을 의미합니다.

하지만 이 경우처럼 순환이 아닌 일반적인(conventional) 컨볼루션에 이 정리를 적용하려면, 입력 시퀀스에 제로 패딩(zero padding)을 먼저 추가한 후, 결과에서 다시 패딩을 제거(unpadding)하는 과정이 필요합니다.

입력 시퀀스의 길이가 길어질수록, 이 방식은 직접 계산하는 컨볼루션보다 훨씬 효율적입니다.

정리하면,
- 한번에 y를 계산할 수 있는 필터 커널 $\bar{K}$가 있습니다.
- 커널 $\bar{K}$는 길이 $L$인 필터입니다.
- 이 필터를 이용해 입력 시퀀스 $u$와 컨볼루션 연산을 수행하면 출력 $y$를 얻을 수 있습니다.
- 컨볼루션 연산 대신 FFT를 사용하면 더욱 빠르게 필터 연산을 수행할 수 있습니다. 

---
## S4 layer 구현하기
이제 진짜 뉴럴 네트워크를 구현해봅시다. 

#### SSM Neural Network란?

SSM은 시계열 입력 $u \in \mathbb{R}^L$을 받아 출력 $y \in \mathbb{R}^L$을 생성하는 1D-1D sequence mapping 구조입니다. 이 구조에서는 내부적으로 다음 파라미터들을 학습합니다:
- System matrix: $A$
- Input/output matrix: $B$, $C$
- Step size: $\Delta$ (learn in log space)

> SSM layer의 핵심은 필터를 구현하는 것입니다.

#### SSMLayer 구현

```python

class SSMLayer(nn.Module):
    N: int  # A 행렬의 차원의 크기
    l_max: int  # 최대 시퀀스 길이
    decode: bool = False

    def setup(self):
        self.A = self.param("A", lecun_normal(), (self.N, self.N))
        self.B = self.param("B", lecun_normal(), (self.N, 1))
        self.C = self.param("C", lecun_normal(), (1, self.N))
        self.D = self.param("D", nn.initializers.ones, (1,))
        self.log_step = self.param("log_step", log_step_initializer(), (1,))

        step = np.exp(self.log_step)
        self.ssm = discretize(self.A, self.B, self.C, step=step)
        self.K = K_conv(*self.ssm, self.l_max)

        # RNN 캐시 (Flax 변수 컬렉션 사용)
        self.x_k_1 = self.variable("cache", "cache_x_k", np.zeros, (self.N,))

    def __call__(self, u):
        if not self.decode:
            return causal_convolution(u, self.K) + self.D * u
        else:
            x_k, y_s = scan_SSM(*self.ssm, u[:, np.newaxis], self.x_k_1.value)
            if self.is_mutable_collection("cache"):
                self.x_k_1.value = x_k
            return y_s.reshape(-1).real + self.D * u
```

> setup은 파라미터가 업데이트 될 때마다 호출되는 함수입니다. 
- CNN 모드(train mode): 커널 $K$를 이용한 전체 시퀀스 컨볼루션
- RNN 모드(decode mode): scan_SSM으로 순차적으로 상태 업데이트

#### 병렬화: 여러 SSM 레이어 복제

SSM은 기본적으로 scalar input/output을 처리하므로, 여러 개를 병렬로 쌓아야 합니다. Flax의 vmap을 활용해 간단하게 구현할 수 있습니다:

```python
def cloneLayer(layer):
    return nn.vmap(
        layer,
        in_axes=1,
        out_axes=1,
        variable_axes={"params": 1, "cache": 1, "prime": 1},
        split_rngs={"params": True},
    )

SSMLayer = cloneLayer(SSMLayer)
```

#### SequenceBlock: SSM + Dropout + Projection
이 블록은 하나의 SSM 레이어와 정규화, 드롭아웃, 선형 투영을 포함한 Transformer-style 구조입니다.

```python 
class SequenceBlock(nn.Module):
    ...
    def setup(self):
        self.seq = self.layer_cls(**self.layer, decode=self.decode)
        self.norm = nn.LayerNorm()
        self.out = nn.Dense(self.d_model)
        if self.glu:
            self.out2 = nn.Dense(self.d_model)
        self.drop = nn.Dropout(...)

    def __call__(self, x):
        skip = x
        if self.prenorm:
            x = self.norm(x)
        x = self.seq(x)
        x = self.drop(nn.gelu(x))
        if self.glu:
            x = self.out(x) * jax.nn.sigmoid(self.out2(x))
        else:
            x = self.out(x)
        x = skip + self.drop(x)
        if not self.prenorm:
            x = self.norm(x)
        return x
```

#### SSM layer 쌓기
Layer Stacking의 목적은 다음과 같습니다:

1. 표현력 강화
→ 여러 layer를 통과하면서 더 복잡하고 비선형적인 패턴을 학습할 수 있음
→ SSM + 비선형 활성함수 (예: GeLU) + skip connection → 깊은 함수 근사

2. 추상화 수준 확장
→ 낮은 layer: 저수준 패턴 (예: 로컬 반복, 리듬)
→ 높은 layer: 고수준 의미 구조 (예: 문장 구조, 이벤트 흐름)

3. 시간적 receptive field 확장
→ 각 SSM 레이어는 제한된 길이의 커널을 가짐
→ 여러 층을 쌓아야 더 긴 의존성을 캡처할 수 있음

4. Transformer처럼 block-wise 구성
→ SSMLayer + Dropout + Dense + Residual → SequenceBlock
→ 이걸 여러 개 쌓아서 MLP처럼 깊게 모델 구성 가능


```python
class StackedModel(nn.Module):
    ...
    def setup(self):
        self.encoder = Embedding(...) if self.embedding else nn.Dense(self.d_model)
        self.decoder = nn.Dense(self.d_output)
        self.layers = [
            SequenceBlock(...)
            for _ in range(self.n_layers)
        ]

    def __call__(self, x):
        if not self.classification:
            if not self.embedding:
                x = x / 255.0
            if not self.decode:
                x = np.pad(x[:-1], [(1, 0), (0, 0)])
        x = self.encoder(x)
        for layer in self.layers:
            x = layer(x)
        if self.classification:
            x = np.mean(x, axis=0)
        x = self.decoder(x)
        return nn.log_softmax(x, axis=-1)
```

#### 배치 차원를 위한 vmap 래핑
Flax에서는 배치 차원을 명시적으로 vmap으로 감쌉니다. RNN 캐시와 파라미터 등의 변수 컬렉션을 명확히 나눠줘야 합니다.

```python
BatchStackedModel = nn.vmap(
    StackedModel,
    in_axes=0,
    out_axes=0,
    variable_axes={"params": None, "dropout": None, "cache": 0, "prime": None},
    split_rngs={"params": False, "dropout": True},
)
```

- 이 모델은 (batch size, sequence length, hidden dimension) 형태의 sequence-to-sequence 매핑 함수를 정의합니다.
- 이는 Transformer, RNN, CNN 등 다른 시퀀스 모델들과 동일한 입출력 구조를 따릅니다.
- 모델 학습을 위한 전체 코드는 [train.py](https://github.com/srush/annotated-s4/blob/main/s4/train.py)에 정의되어 있습니다.

---

## Issue
> 그러나 문제가 있습니다.

1. 랜덤으로 초기화된 SSM은 학습 성능이 좋지 않습니다.
2. 커널 $K$를 naive하게 계산하면, 연산량이 많고 메모리 사용량 또한 비효율적입니다. 특히 시퀀스 길이가 길어질수록 성능 저하가 커집니다.

이 두가지 문제를 S4가 어떻게 해결했는지는 다음 글에서 알아보도록 하겠습니다. 